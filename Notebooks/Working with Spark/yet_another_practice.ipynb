{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lower, when, concat, lit, length, regexp_extract, avg, max, min, sum, desc\n",
    "\n",
    "# Initialize Spark session (already running in Databricks)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV data into Spark from Databricks FileStore\n",
    "file_path = \"dbfs:/FileStore/tables/session_info_colab.csv\"  # Adjust the path if necessary\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "# Initial Data Inspection\n",
    "print(\"Initial Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"Initial Data:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Data Cleaning and Preprocessing\n",
    "cleaned_df = df \\\n",
    "    .withColumn(\"user_name\", lower(col(\"user_name\"))) \\\n",
    "    .withColumn(\"email_domain\", regexp_extract(col(\"email\"), r'@(.+)', 1)) \\\n",
    "    .filter(col(\"email\").isNotNull()) \\\n",
    "    .dropDuplicates([\"user_id\"]) \\\n",
    "    .withColumn(\"address_length\", length(col(\"address\")))\n",
    "\n",
    "# Transformations and New Columns\n",
    "transformed_df = cleaned_df \\\n",
    "    .withColumn(\"status\", when(col(\"address_length\") > 50, lit(\"Detailed\")).otherwise(lit(\"Short\"))) \\\n",
    "    .withColumn(\"full_user_info\", concat(col(\"user_name\"), lit(\" - \"), col(\"email_domain\")))\n",
    "\n",
    "# Aggregations\n",
    "user_stats = cleaned_df.groupBy(\"user_name\").agg(\n",
    "    count(\"*\").alias(\"session_count\"),\n",
    "    avg(\"address_length\").alias(\"avg_address_length\"),\n",
    "    max(\"address_length\").alias(\"max_address_length\"),\n",
    "    min(\"address_length\").alias(\"min_address_length\")\n",
    ")\n",
    "\n",
    "# Sorting Example\n",
    "sorted_user_stats = user_stats.orderBy(desc(\"session_count\"))\n",
    "\n",
    "# Joins\n",
    "joined_df = transformed_df.join(user_stats, on=\"user_name\", how=\"inner\")\n",
    "\n",
    "# Caching Example\n",
    "joined_df.cache()\n",
    "print(f\"Count after caching: {joined_df.count()}\")\n",
    "\n",
    "# Grouped Aggregations and Additional Insights\n",
    "email_domain_stats = cleaned_df.groupBy(\"email_domain\").agg(\n",
    "    count(\"user_id\").alias(\"user_count\"),\n",
    "    sum(\"address_length\").alias(\"total_address_length\")\n",
    ").orderBy(desc(\"user_count\"))\n",
    "\n",
    "# Data Insights\n",
    "print(\"Transformed Data:\")\n",
    "transformed_df.show(5, truncate=False)\n",
    "\n",
    "print(\"User Statistics:\")\n",
    "sorted_user_stats.show(5, truncate=False)\n",
    "\n",
    "print(\"Joined Data:\")\n",
    "joined_df.show(5, truncate=False)\n",
    "\n",
    "print(\"Email Domain Statistics:\")\n",
    "email_domain_stats.show(5, truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
