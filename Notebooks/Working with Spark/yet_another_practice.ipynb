{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lower, when, concat, lit, length, regexp_extract, avg, max, min, sum, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session (already running in Databricks)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkDataProcessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV data into Spark from Databricks FileStore\n",
    "file_path = \"dbfs:/FileStore/tables/session_info_colab.csv\"  # Adjust the path if necessary\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initial Data Inspection\n",
    "print(\"Initial Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Initial Data:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "cleaned_df = df \\\n",
    "    .withColumn(\"user_name\", lower(col(\"user_name\"))) \\\n",
    "    .withColumn(\"email_domain\", regexp_extract(col(\"email\"), r'@(.+)', 1)) \\\n",
    "    .filter(col(\"email\").isNotNull()) \\\n",
    "    .dropDuplicates([\"user_id\"]) \\\n",
    "    .withColumn(\"address_length\", length(col(\"address\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transformations and New Columns\n",
    "transformed_df = cleaned_df \\\n",
    "    .withColumn(\"status\", when(col(\"address_length\") > 50, lit(\"Detailed\")).otherwise(lit(\"Short\"))) \\\n",
    "    .withColumn(\"full_user_info\", concat(col(\"user_name\"), lit(\" - \"), col(\"email_domain\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregations\n",
    "user_stats = cleaned_df.groupBy(\"user_name\").agg(\n",
    "    count(\"*\").alias(\"session_count\"),\n",
    "    avg(\"address_length\").alias(\"avg_address_length\"),\n",
    "    max(\"address_length\").alias(\"max_address_length\"),\n",
    "    min(\"address_length\").alias(\"min_address_length\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sorting\n",
    "sorted_user_stats = user_stats.orderBy(desc(\"session_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Joins\n",
    "joined_df = transformed_df.join(user_stats, on=\"user_name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Caching Example\n",
    "joined_df.cache()\n",
    "print(f\"Count after caching: {joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Grouped Aggregations and Additional Insights\n",
    "email_domain_stats = cleaned_df.groupBy(\"email_domain\").agg(\n",
    "    count(\"user_id\").alias(\"user_count\"),\n",
    "    sum(\"address_length\").alias(\"total_address_length\")\n",
    ").orderBy(desc(\"user_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Insights\n",
    "print(\"Transformed Data:\")\n",
    "transformed_df.show(5, truncate=False)\n",
    "\n",
    "print(\"User Statistics:\")\n",
    "sorted_user_stats.show(5, truncate=False)\n",
    "\n",
    "print(\"Joined Data:\")\n",
    "joined_df.show(5, truncate=False)\n",
    "\n",
    "print(\"Email Domain Statistics:\")\n",
    "email_domain_stats.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use `spark.stop()`?\n",
    "\n",
    "The `spark.stop()` method is used to **gracefully shut down a SparkSession or SparkContext** after completing your Spark operations. Below are the key reasons why this is important:\n",
    "\n",
    "#### Key Reasons to Use `spark.stop()`:\n",
    "\n",
    "1. **Release Resources**:\n",
    "   - Spark consumes significant resources such as memory, disk, and CPU. \n",
    "   - Calling `spark.stop()` ensures these resources are released when they are no longer needed.\n",
    "\n",
    "2. **Avoid Resource Contention**:\n",
    "   - If `spark.stop()` is not called, the Spark application might continue holding resources.\n",
    "   - This can cause issues when running multiple Spark jobs or sharing a cluster.\n",
    "\n",
    "3. **Allow Subsequent Jobs to Run**:\n",
    "   - Ensures that subsequent Spark jobs start with a clean slate.\n",
    "   - Prevents conflicts or errors caused by residual sessions or contexts.\n",
    "\n",
    "4. **Prevent Memory Leaks**:\n",
    "   - Not stopping the Spark session may lead to memory leaks, especially in long-running applications or interactive sessions like Jupyter Notebooks.\n",
    "\n",
    "5. **Best Practice**:\n",
    "   - Similar to closing a file after opening it, stopping the Spark session is a good resource management practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
