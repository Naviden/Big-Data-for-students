{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cd3ab9",
   "metadata": {},
   "source": [
    "\n",
    "# Explanation of Functions Used in the Airbnb Notebook\n",
    "\n",
    "In this notebook, we will explain the key functions used in the Airbnb analysis, demonstrated with a simple dataset about people and their salary. The dataset contains information about people's salary, department, and years of experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e746f",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Loading a CSV File into a Spark DataFrame\n",
    "\n",
    "The first step in any Spark analysis is to load the data. Here's how to load a CSV file into a Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Salary Analysis\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a Spark DataFrame\n",
    "file_path = \"/dbfs/FileStore/tables/people_salary_data.csv\"  # Adjust path if needed\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611c618",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Filtering Data\n",
    "\n",
    "The `filter()` function is used to filter rows based on a condition. For example, we can filter people whose salary is greater than $60,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b81bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter rows where salary is greater than 60,000\n",
    "high_salary_df = df.filter(df.salary > 60000)\n",
    "high_salary_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6239395",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Adding a New Column\n",
    "\n",
    "We can use the `withColumn()` function to add a new column. In this case, we'll add a column that categorizes people into salary tiers: Low, Medium, and High.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1822a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Add a new column 'salary_tier' based on the salary\n",
    "df = df.withColumn(\"salary_tier\", \n",
    "                   when(df.salary < 60000, \"Low\")\n",
    "                   .when((df.salary >= 60000) & (df.salary <= 70000), \"Medium\")\n",
    "                   .otherwise(\"High\"))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a6080",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Aggregating Data\n",
    "\n",
    "The `groupBy()` function is used to group data by one or more columns. We can then use aggregation functions like `avg()` to calculate the average salary in each department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6063ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group data by 'department' and calculate the average salary\n",
    "avg_salary_by_department = df.groupBy(\"department\").avg(\"salary\")\n",
    "avg_salary_by_department.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd902b10",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Data Cleaning (Handling Null Values)\n",
    "\n",
    "To clean the data, we can use functions like `fillna()` to replace `null` values with a default value. For example, if the 'experience_years' column has missing values, we can fill them with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill missing values in 'experience_years' with 0\n",
    "cleaned_df = df.fillna({'experience_years': 0})\n",
    "cleaned_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
